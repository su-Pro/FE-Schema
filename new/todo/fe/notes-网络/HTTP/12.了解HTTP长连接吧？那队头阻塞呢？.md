## 了解 HTTP 长连接吧？那队头阻塞呢？

### 连接管理

![20200610214303]( https://supyyy-1259673491.cos.ap-beijing.myqcloud.com/2020/pictures20200610214303.png)

#### 连接管理：短连接

HTTP 协议最初(0.9/1.0)是个非常简单的协议,通信过程也采用了简单的“请求 - 应答”方式。

它底层的数据传输基于 TCP/IP,每次发送请求前需要先与服务器建立连接,收到响应报文后会立即关闭连接。每次 HTTP 请求都需要 TCP 建立连接（1 个 RTT），以及关闭连接（2 个 RTT）共 3 个 RTT。

#### 连接管理：长连接

针对短连接暴露出的缺点,HTTP 协议就提出了“长连接”的通信方式,也叫“持久连接”(persistent connections)、“连接保活”(keep alive)、“连接复用”(connection reuse)。

**基于“成本均摊”的思路**，既然 TCP 的连接和关闭非常耗时间,那么就把这个时间成本由原来的一个“请求 - 应答”均摊到多个“请求 - 应答”上。
这样虽然不能改善 TCP 的连接效率,但基于“分母效应”,每个“请求 - 应答”的无效时间就会降低不少,整体传输效率也就提高了。

#### **长连接的缺点**

因为 TCP 连接长时间不关闭,服务器必须在内存里保存它的状态,这就**占用了服务器的资源**。

如果有大量的空闲长连接只连不发,就会很快耗尽服务器的资源,导致服务器无法为真正有需要的用户提供服务。

> 因此长连接也需要在恰当的时间关闭,不能永远保持与服务器的连接。

#### 关闭长连接

客户端关闭方式：可以在请求头里加上“Connection: close”字段,告诉服务器:“这次通信后就关闭连接”。

服务器端关闭方式：通常不会主动关闭连接,但也可以使用一些策略。拿 Nginx 来举例,它有两种方式:

1. 使用“keepalive_timeout”指令，设置长连接的超时时间，如果在一段时间内连接上没有任何数据收发就主动断开连接，避免空闲连接占用系统资源。
2. 使用“keepalive_requests”指令，设置长连接上可发送的最大请求次数。比如设置成 1000，那么当 Nginx 在这个连接上处理了 1000 个请求后，也会主动断开连接。

> 不过不管客户端是否显式要求长连接,如果服务器支持长连接,它总会在响应报文里放一个“Connection: keep- alive”字段，HTTP1.1 是默认支持长连接的

### 队头阻塞

“队头阻塞”与短连接和长连接无关,而是由 HTTP 基本的“请求 - 应答”模型所导致的。在通信管道中形成了一个先进先出的“串行”队列，队列里的请求没有轻重缓急的优先级,只有入队的先后顺序,排在最前面的请求被最优先处理。

![20200610214318]( https://supyyy-1259673491.cos.ap-beijing.myqcloud.com/2020/pictures20200610214318.png)

#### 缓解队头阻塞

1. “并发连接”：同时对一个域名发起多个长连接, 用数量来解决质量的问题。但这种方式也存在缺陷。如果每个客户端都想自己快,建立很多个连接,用户数 × 并发数就会是个天文数字。服务器的资源根本就扛不住,或者被服务器认为是恶意攻击,反而会造成“拒绝服务”

> RFC2616 里明确限制每个客户端最多并发 2 个连接。不过实践证明这个数字实在是太小了,众多浏览器都“无视”标准,把这个上限提高到了 6~8。后来修订的 RFC7230 也就“顺水推舟”,取消了这个“2”的限制.

2. “域名分片”HTTP 协议和浏览器不是限制并发连接数量吗?好,那我就多开几个域名,比如 `shard1.test.com`、`shard2.test.com`,而这些域名都指向同一台服务器`www.test.com`,这样实际长连接的数量就又上去了.

3. 升级到 HTTP2

> 由于 HTTP2 采用二进制格式传输 header + body，通过流实现了多路复用，从根本上解决了 HTTP 层面的队头阻塞。
